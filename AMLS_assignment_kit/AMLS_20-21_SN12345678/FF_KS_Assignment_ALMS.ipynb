{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLS_assignment20_21/ SN:14066539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you come across memory issue, my advise would be to change Target_size in the relevant files (MA1, MA2, MB1, MB2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importin required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as v1\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from A1 import MA1\n",
    "from B1 import MB1\n",
    "from A2 import MA2\n",
    "from B2 import MB2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping of directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir, image_dir, label_dir, asset_dir\n",
    "\n",
    "data_dir = ('Datasets')\n",
    "image_dir_celeba = os.path.join(data_dir, 'celeba', 'img')\n",
    "image_dir_cartoonset = os.path.join(data_dir,'cartoon_set', 'img')\n",
    "asset_dir_celeba = os.listdir(os.path.join(os.getcwd(), 'Datasets/celeba/img'))\n",
    "asset_dir_cartoonset = os.listdir(os.path.join(os.getcwd(), 'Datasets/cartoon_set/img'))\n",
    "label_dir_celeba = os.path.join(data_dir,'celeba', 'labels.csv')\n",
    "label_dir_cartoonset = os.path.join(data_dir, 'cartoon_set', 'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting directories global for use across multiple models\n",
    "global data_dir, image_dir_celeba, image_dir_cartoonset, asset_dir_celeba, labels_dir_celeba, labels_dir_cartoonset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA1 k-Means & MLP model for gender recognition : celeba dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This model has used k-Means clustering algorithm followed by MultiLayer Perceptron model to train and predict the Gender labels of the Celeba dataset. I chose to use kMeans and generate features, as I stumbled upon multiple technical challenges importing dlib used in Week 7 of the lectures. Following a literature review I was convinced of k-Means algorithm being a good substitute for the features extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 images from celeba dataset was initially conversted to train MultiLayer Perceptron model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running k_Means function in order to get centroids coordinates with relevant arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify criteria\n",
    "max_iters = 10         # Number of iterations, ideal to keep it <10 \n",
    "samples = 100          # It is recommended to keep this number small (in the range of 10 - 30), as this calculation is computationally expensive process.\n",
    "K = 20                 # Number of centroids. Not all the images high dimension data available to slice, hence as per my observation, a number between 10 and 50 is ideal for this dataset.\n",
    "target_size = (79, 59)  # multiply this three times to make it actual size.\n",
    "\n",
    "image_dir_celeba = os.path.join(data_dir, 'celeba', 'img')\n",
    "asset_dir_celeba = os.listdir(os.path.join(os.getcwd(), 'Datasets/celeba/img'))\n",
    "\n",
    "# Get the centroids of the sample images --------------------\n",
    "centroid_array, idx_array = MA1.get_centroids_celebaMA1(asset_dir_celeba, image_dir_celeba, samples, K, max_iters, target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Please ignore the alert of empty slice. It seems to me that the dataset was already optimized, hence the error indicates lack of centroids in different dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Shapping and loading data for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting shapped data for Model A1:\n",
    "\n",
    "number_labels = len(set(pd.read_csv(label_dir_celeba, delimiter= '\\t' )['gender']))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, features = MA1.get_data_MA1(number_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up MLP with effective parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "n_hidden_1 = 2048 # can change this number to optimize results\n",
    "n_hidden_2 = 2048 # can change this number to optimize results\n",
    "\n",
    "# Setting up learning rates and number of epochs for start\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 500\n",
    "display_accuracy_step = 10\n",
    "\n",
    "weights, X, Y, biases, input_array = MA1.initialize_parameters(X_train, Y_train, n_hidden_1, n_hidden_2)\n",
    "\n",
    "logits = MA1.multilayer_perceptron(weights, biases, input_array)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "\n",
    "optimizer = v1.train.AdamOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Running MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "\n",
    "init_op = v1.global_variables_initializer() # initializing global parameter\n",
    "\n",
    "with v1.Session() as sess:\n",
    "\n",
    "    # run graph weights/biases initialization op\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # running cost function (backprop)\n",
    "        _, cost = sess.run([optimizer, loss_op], feed_dict={X: X_train, Y: Y_train})\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Plotting results\n",
    "        \n",
    "        if epoch % display_accuracy_step == 0:\n",
    "            predict = tf.nn.softmax(logits)  # Softmax\n",
    "            correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))\n",
    "            # on using argmax to predict accuracy on axis = 1\n",
    "            # calculate training accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "            test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "    # Plotting results\n",
    "    \n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    print (\" .. Model Finished ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of k-Means compression on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Experiment with these parameters ================\n",
    "K = 24\n",
    "max_iters = 10\n",
    "\n",
    "# Change the file name and path to experiment with different images\n",
    "img_sample = os.path.join(image_dir_celeba, '1.jpg')\n",
    "A = image.img_to_array(image.load_img(img_sample, target_size = None , interpolation='bicubic'))\n",
    "\n",
    "# Divide by 255 so that all values are in the range 0 - 1\n",
    "A = A / 255\n",
    "X = A.reshape(-1, 3)\n",
    "\n",
    "initial_centroids = MA1.kMeansInitCentroids(X, K)\n",
    "centroids, idx = MA1.runkMeans(X, initial_centroids,\n",
    "                                 MA1.findClosestCentroids,\n",
    "                                 MA1.computeCentroids,\n",
    "                                 max_iters)\n",
    "\n",
    "X_recovered = centroids[idx, :].reshape(A.shape)\n",
    "\n",
    "\n",
    "# Display the original image, rescale back by 255\n",
    "fig, ax = pyplot.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].imshow(A)\n",
    "ax[0].set_title('Original')\n",
    "ax[0].grid(False)\n",
    "\n",
    "# Display compressed image, rescale back by 255\n",
    "ax[1].imshow(X_recovered)\n",
    "ax[1].set_title('Compressed, with %d colors' % K)\n",
    "ax[1].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA2 Principal Component Analysis (PCA) & MLP for Smiling recognition : celeba dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model focuses on analysing Celeba datasets using Principal Component. \n",
    "First stage of implementation of this algorithm is to find the correct set of Principal Component (eigenfaces) that describes the largest variations of the image. This can be changed by increasing K in this model. But due to the computational restrictions, I have used 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is implemented in two steps.\n",
    "(i) first we normalize the dataset by subtracting the mean of each features from the matrix.\n",
    "(ii)In this stage we can use normalized values to reduce the dimesnsion fo the dataset. Which allows learning algorithm with a smaller input size.\n",
    "\n",
    "In this Model I have used K = 128. The K here was derived using covariance matrix greater than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA compression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_compression(K, number_labels): # mention of K depending on the image resolution and criteria\n",
    "    \n",
    "    X, Y = MA2.data_to_vector_MA2 (asset_dir_celeba, number_labels)\n",
    "    \n",
    "    X_norm, mu, sigma = MA2.featureNormalize(X)\n",
    "    U, S = MA2.pca(X_norm)\n",
    "    X = MA2.projectData(X_norm, U, K)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, X_val, Y_val = MA2.split_data(X, Y)\n",
    "    print('Shape of compressed X_train:', X_train.shape)\n",
    "    print('Shape of compressed X_test:', X_test.shape)\n",
    "    print('Shape of Y_train:', Y_train.shape)\n",
    "    print('Shape of Y_test:', Y_test.shape)\n",
    "    print('Shape of X_val:', X_val.shape)\n",
    "    print('Shape of Y_val:', Y_val.shape)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X_val, Y_val, U, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Compressing and loading compressed image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 128    # I have chosen value of 128 as it gives high degree of accuracy perhaps way too much for this purpose :)\n",
    "\n",
    "number_labels = len(set(pd.read_csv(label_dir_celeba, delimiter= '\\t' )['smiling']))\n",
    "\n",
    "# Current Image size is set to (79, 59), this will need to change as required in file MA2.py (function: data_to_vector_MA2)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, X_val, Y_val, U, S = PCA_compression(K, number_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up MLP with basic arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "n_hidden_1 = 2048 # can change this number to optimize results\n",
    "n_hidden_2 = 2048 # can change this number to optimize results\n",
    "\n",
    "# Setting up learning rates and number of epochs for start\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 200\n",
    "display_accuracy_step = 10\n",
    "\n",
    "weights, X, Y, biases, input_array = MA2.initialize_parameters(X_train, Y_train, n_hidden_1, n_hidden_2)\n",
    "\n",
    "logits = MA2.multilayer_perceptron(weights, biases, input_array)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "\n",
    "optimizer = v1.train.AdamOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Running MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs = []\n",
    "\n",
    "init_op = v1.global_variables_initializer() # initializing global parameter\n",
    "\n",
    "with v1.Session() as sess:\n",
    "\n",
    "    # run graph weights/biases initialization op\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # running cost function (backprop)\n",
    "        _, cost = sess.run([optimizer, loss_op], feed_dict={X: X_train, Y: Y_train})\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Plotting results\n",
    "        \n",
    "        if epoch % display_accuracy_step == 0:\n",
    "            predict = tf.nn.softmax(logits)  # Softmax\n",
    "            correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))\n",
    "            # on using argmax to predict accuracy on axis = 1\n",
    "            # calculate training accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "            val_accuracy = accuracy.eval({X: X_val, Y: Y_val})\n",
    "            test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "    # Plotting results\n",
    "    \n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Val Accuracy:\", val_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    print (\" .. Model Finished ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MB1 PCA & MLP for eye-color recognition : cartoon_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First stage of implementation of this algorithm is to find the correct set of Principal Component (eigenfaces) that describes the largest variations of the image. This can be changed by increasing K in this model. But due to the computational restrictions, I have used 128."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is implemented in two steps across both datasets.\n",
    "(i) first we normalize the dataset by subtracting the mean of each features from the matrix.\n",
    "(ii)In this stage we can use normalized values to reduce the dimesnsion fo the dataset. Which allows learning algorithm with a smaller input size.\n",
    "\n",
    "In this Model I have used K = 128. The K here was derived using covariance matrix greater than 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PCA compression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_compression(K, number_labels): # mention of K depending on the image resolution and criteria\n",
    "    \n",
    "    X, Y = MB1.data_to_vector_MB1 (asset_dir_cartoonset, number_labels)\n",
    "    \n",
    "    X_norm, mu, sigma = MB1.featureNormalize(X)\n",
    "    U, S = MB1.pca(X_norm)\n",
    "    X = MB1.projectData(X_norm, U, K)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test, X_val, Y_val = MB1.split_data(X, Y)\n",
    "    print('Shape of compressed X_train:', X_train.shape)\n",
    "    print('Shape of compressed X_test:', X_test.shape)\n",
    "    print('Shape of Y_train:', Y_train.shape)\n",
    "    print('Shape of Y_test:', Y_test.shape)\n",
    "    print('Shape of X_val:', X_val.shape)\n",
    "    print('Shape of Y_val:', Y_val.shape)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X_val, Y_val, U, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Compressing and loading compressed image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K = 128    # I have chosen value of 128 as it gives high degree of accuracy perhaps way too much for this purpose :)\n",
    "\n",
    "number_labels = len(set(pd.read_csv(label_dir_cartoonset, delimiter= '\\t' )['eye_color']))\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, X_val, Y_val, U, S = PCA_compression(K, number_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up MLP with basic arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "n_hidden_1 = 2048 # can change this number to optimize results\n",
    "n_hidden_2 = 2048 # can change this number to optimize results\n",
    "\n",
    "# Setting up learning rates and number of epochs for start\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 2000\n",
    "display_accuracy_step = 10\n",
    "\n",
    "weights, X, Y, biases, input_array = MB1.initialize_parameters(X_train, Y_train, n_hidden_1, n_hidden_2)\n",
    "\n",
    "logits = MB1.multilayer_perceptron(weights, biases, input_array)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Running MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs = []\n",
    "validation = []\n",
    "\n",
    "init_op = v1.global_variables_initializer() # initializing global parameter\n",
    "\n",
    "with v1.Session() as sess:\n",
    "\n",
    "    # run graph weights/biases initialization op\n",
    "    sess.run(init_op)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # running cost function (backprop)\n",
    "        _, cost = sess.run([optimizer, loss_op], feed_dict={X: X_train, Y: Y_train})\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Plotting results\n",
    "        \n",
    "        if epoch % display_accuracy_step == 0:\n",
    "            predict = tf.nn.softmax(logits)  # Softmax\n",
    "            correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))\n",
    "            # on using argmax to predict accuracy on axis = 1\n",
    "            # calculate training accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "            val_accuracy = accuracy.eval({X: X_val, Y: Y_val})\n",
    "            test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "    # Plotting results\n",
    "    \n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(np.squeeze(validation))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    print (\" .. Model Finished ..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MB2 K-mean & MLP for face-shape recognition : cartoon_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Running k-Means in order to compress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify criteria\n",
    "max_iters = 6     # Number of iterations, \n",
    "samples = 1000     # It is recommended to keep this number small but could go as large as the whole dataset. This calculation is computationally expensive process.\n",
    "K = 12            # Number of centroids. Not all the images high dimension data available to slice, hence as per my observation, a number between 10 and 50 is ideal for this dataset.\n",
    "target_size = (150,150)\n",
    "\n",
    "# Get the centroids of the sample images --------------------\n",
    "cartoon_centroid_array, cartoon_idx_array = MB2.get_centroids_cartoonMB2(asset_dir_cartoonset, image_dir_cartoonset, samples, K, max_iters, target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Shapping and loading data to pass it on to MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting shapped data for Model A1:\n",
    "\n",
    "number_labels = len(set(pd.read_csv(label_dir_cartoonset, delimiter= '\\t')['face_shape'])) # Male or Female\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, features = MB2.get_data_MB2(number_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up MLP with effective parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "n_hidden_1 = 2048 # can change this number to optimize results\n",
    "n_hidden_2 = 2048 # can change this number to optimize results\n",
    "\n",
    "# Setting up learning rates and number of epochs for start\n",
    "learning_rate = 1e-5\n",
    "training_epochs = 1000\n",
    "display_accuracy_step = 10\n",
    "\n",
    "weights, X, Y, biases, input_array = MA1.initialize_parameters(X_train, Y_train, n_hidden_1, n_hidden_2)\n",
    "\n",
    "logits = MA1.multilayer_perceptron(weights, biases, input_array)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "\n",
    "optimizer = v1.train.AdamOptimizer(learning_rate).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1  Running MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "costs = []\n",
    "\n",
    "init_op = v1.global_variables_initializer() # initializing global parameter\n",
    "\n",
    "with v1.Session() as sess:\n",
    "\n",
    "    # run graph weights/biases initialization op\n",
    "    sess.run(init_op) \n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # running cost function (backprop)\n",
    "        _, cost = sess.run([optimizer, loss_op], feed_dict={X: X_train, Y: Y_train})\n",
    "        costs.append(cost)\n",
    "\n",
    "        # Plotting results\n",
    "        \n",
    "        if epoch % display_accuracy_step == 0:\n",
    "            predict = tf.nn.softmax(logits)  # Softmax\n",
    "            correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(Y, 1))\n",
    "            # on using argmax to predict accuracy on axis = 1\n",
    "            # calculate training accuracy\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "            train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "            test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "\n",
    "    # Plotting results\n",
    "    \n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    print (\" .. Model Finished ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
